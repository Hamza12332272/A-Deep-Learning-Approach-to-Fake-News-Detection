main. py 
import os
from src.preprocess import load_data, preprocess_data
from src.train import train_model
from src.evaluate import evaluate_model
from src.model import build_model

if __name__ == "__main__":
    # Define dataset paths
    fake_path = r'C:\Users\Startklar\PycharmProjects\Deep Learning Project\Data\fake.csv'
    true_path = r'C:\Users\Startklar\PycharmProjects\Deep Learning Project\Data\true.csv'

    # Load and preprocess the data
    print("Loading and preprocessing data...")
    dataset = load_data(fake_path, true_path)
    dataset = preprocess_data(dataset)

    # Model hyperparameters
    vocab_size = 10000
    embedding_dim = 128
    max_sequence_length = 500

    # Create the model
    print("Creating the model...")
    model = build_model(vocab_size, embedding_dim, max_sequence_length)

    # Train the model
    print("Training the model...")
    model, X_test, y_test = train_model(model, dataset, max_sequence_length)

    # Evaluate the model
    print("Evaluating the model...")
    evaluate_model(model, X_test, y_test)

model.py 
import tensorflow as tf
from tensorflow.keras import layers


def build_model(vocab_size, embedding_dim, max_sequence_length):
    """
    Builds a sequential neural network model for text classification.

    Args:
        vocab_size (int): Size of the vocabulary for embedding.
        embedding_dim (int): Dimension of the embedding vectors.
        max_sequence_length (int): Maximum length of input sequences.

    Returns:
        tf.keras.Model: Compiled text classification model.
    """
    model = tf.keras.Sequential([
        # Embedding layer: Maps words in the vocabulary to dense vectors of fixed size.
        layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),

        # CNN (Conv1D): Extracts local features from the input sequence.
        # Useful for capturing n-gram features, where kernel_size determines the n-gram size.
        layers.Conv1D(filters=128, kernel_size=5, activation='relu'),

        # MaxPooling1D: Reduces the dimensionality of the feature map from Conv1D.
        # Helps retain the most important features while reducing computational complexity.
        layers.MaxPooling1D(pool_size=2),

        # Bidirectional LSTM: Captures temporal relationships and dependencies in the text.
        # Bidirectional layer processes the sequence in both forward and backward directions.
        layers.Bidirectional(layers.LSTM(64)),

        # Dense Layer: Fully connected layer for learning high-level abstract features.
        layers.Dense(64, activation='relu'),

        # Dropout: Prevents overfitting by randomly setting a fraction of input units to zero during training.
        layers.Dropout(0.5),

        # Output Layer: Sigmoid activation for binary classification (Fake News or Real News).
        layers.Dense(1, activation='sigmoid')
    ])

    # Build the model with a specified input shape.
    model.build(input_shape=(None, max_sequence_length))

    # Display the model summary to understand layer connections and parameter counts.
    model.summary()

    return model

__init__.py
# src/__init__.py

# Import core functions and classes to make them available directly from the package
from .preprocess import load_data, preprocess_data
from .train import train_model
from .model import build_model
from .evaluate import evaluate_model



# Optional: Initialize logging for the package
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
logger.info("Package initialized: src")

preprocess.py
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

def load_data(fake_path, true_path):
    fake_data = pd.read_csv(fake_path)
    true_data = pd.read_csv(true_path)

    fake_data['label'] = 0
    true_data['label'] = 1

    data = pd.concat([fake_data, true_data]).sample(frac=1).reset_index(drop=True)
    return data

def preprocess_data(data, max_words=10000, max_sequence_length=500):
    tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
    tokenizer.fit_on_texts(data['text'])

    sequences = tokenizer.texts_to_sequences(data['text'])
    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')

    X = padded_sequences
    y = data['label'].values

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    return {
        "X_train": X_train,
        "X_test": X_test,
        "y_train": y_train,
        "y_test": y_test,
        "tokenizer": tokenizer
    }
train.py
from tensorflow.keras.callbacks import EarlyStopping

def train_model(model, dataset, max_sequence_length, epochs=3, batch_size=64):
    """
    Trains the given model on the provided dataset with early stopping.

    Args:
        model (tf.keras.Model): The compiled neural network model.
        dataset (dict): A dictionary containing the training and test datasets.
                        Keys: "X_train", "y_train", "X_test", "y_test".
        max_sequence_length (int): The maximum sequence length for input data.
        epochs (int): The maximum number of epochs for training.
        batch_size (int): The number of samples per training batch.

    Returns:
        model (tf.keras.Model): The trained model with updated weights.
        X_test (np.array): Test input data for evaluation.
        y_test (np.array): True labels for the test data.
    """
    # Extract training and test data from the dataset dictionary.
    X_train = dataset["X_train"]
    y_train = dataset["y_train"]
    X_test = dataset["X_test"]
    y_test = dataset["y_test"]

    # Early stopping callback:
    # Monitors the validation loss and stops training when it doesn't improve for 2 consecutive epochs.
    # It also restores the model weights to the best state observed during training.
    early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)

    # Compile the model:
    # - Optimizer: Adam, which is well-suited for most deep learning tasks.
    # - Loss function: Binary crossentropy, since this is a binary classification task.
    # - Metrics: Accuracy to monitor training and validation performance.
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    # Train the model using the training data:
    # - Validation data is used to monitor performance after each epoch.
    # - Batch size defines the number of samples processed before the model updates.
    # - Early stopping is used to prevent overfitting and save training time.
    model.fit(X_train, y_train, validation_data=(X_test, y_test),
              epochs=epochs, batch_size=batch_size, callbacks=[early_stopping])

    # Return the trained model and test data for further evaluation.
    return model, X_test, y_test

evaluate.py 
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns


def evaluate_model(model, X_test, y_test):
    predictions = (model.predict(X_test) > 0.5).astype("int32")

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(y_test, predictions, target_names=['Fake', 'Real']))

    # Confusion Matrix
    print("\n--- Visualization: Confusion Matrix ---")
    cm = confusion_matrix(y_test, predictions)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

result :
C:\Users\Startklar\AppData\Local\Programs\Python\Python312\python.exe "C:\Users\Startklar\PycharmProjects\Deep Learning Project\main.py" 
2024-12-14 12:09:39.864509: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-12-14 12:09:40.427537: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Loading and preprocessing data...
INFO:src:Package initialized: src
Creating the model...
C:\Users\Startklar\AppData\Roaming\Python\Python312\site-packages\keras\src\layers\core\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.
  warnings.warn(
2024-12-14 12:10:01.940249: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model: "sequential"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ embedding (Embedding)           │ (None, 500, 128)       │     1,280,000 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d (Conv1D)                 │ (None, 496, 128)       │        82,048 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling1d (MaxPooling1D)    │ (None, 248, 128)       │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ bidirectional (Bidirectional)   │ (None, 128)            │        98,816 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (Dense)                   │ (None, 64)             │         8,256 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout (Dropout)               │ (None, 64)             │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_1 (Dense)                 │ (None, 1)              │            65 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 1,469,185 (5.60 MB)
 Trainable params: 1,469,185 (5.60 MB)
 Non-trainable params: 0 (0.00 B)
Training the model...
Epoch 1/3
562/562 ━━━━━━━━━━━━━━━━━━━━ 87s 152ms/step - accuracy: 0.9460 - loss: 0.1275 - val_accuracy: 0.9991 - val_loss: 0.0070
Epoch 2/3
562/562 ━━━━━━━━━━━━━━━━━━━━ 88s 157ms/step - accuracy: 0.9997 - loss: 0.0017 - val_accuracy: 0.9988 - val_loss: 0.0076
Epoch 3/3
562/562 ━━━━━━━━━━━━━━━━━━━━ 83s 147ms/step - accuracy: 0.9999 - loss: 7.1327e-04 - val_accuracy: 0.9989 - val_loss: 0.0101
Evaluating the model...
281/281 ━━━━━━━━━━━━━━━━━━━━ 5s 19ms/step

Classification Report:
              precision    recall  f1-score   support

        Fake       1.00      1.00      1.00      4668
        Real       1.00      1.00      1.00      4312

    accuracy                           1.00      8980
   macro avg       1.00      1.00      1.00      8980
weighted avg       1.00      1.00      1.00      8980


--- Visualization: Confusion Matrix ---

Process finished with exit code 0
